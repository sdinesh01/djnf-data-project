[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Where’s the coal? Export locations & demographics",
    "section": "",
    "text": "This page seeks to present a test workflow for an upcoming Howard Center for Investigative Journalism project on freight trains that haul coal and pollute communities across the country with coal dust.\nStories about particulate pollution in the industrial sector have exposed environmental consequences for the air, earth, and our waterways. This project would take the first comprehensive look at communities along coal transportation routes – particularly relevant in the wake of major freight derailments, contamination events, and discussions about regulating the length and staff numbers aboard freight transport.\nThe potential impact of this project could be increased investment (social and financial) for particulate monitoring sensors along transportation hubs and/or increased industrial regulation for coal transporters. The social impact would be increasing the public awareness of industrial air pollution or providing an explanation for the phenomenon in affected communities.\nThis story would feature visual components like maps and other data graphics and photos and videos of coal trains and transportation routes.\nNext steps include: creating buffers and performing a spatial join to merge our demographic and geographic data. Then, I will take a look at demographic data at coal export locations nationwide. This analysis will precede a future analysis of complete train routes.\nThis page includes two parts: importing Census demographic data and mapping coal export locations. This test workflow will examine three communities where CSX rail lines run: Prince George’s County and Montgomery County in Maryland, and Jefferson County in West Virginia.\n\n\n\n\n# use this block for imports\nlibrary(leaflet) # for mapping\nlibrary(sf) # for mapping\nlibrary(janitor)\nlibrary(tidyverse)\nlibrary(tidycensus)"
  },
  {
    "objectID": "index.html#story-memo",
    "href": "index.html#story-memo",
    "title": "Where’s the coal? Export locations & demographics",
    "section": "",
    "text": "This page seeks to present a test workflow for an upcoming Howard Center for Investigative Journalism project on freight trains that haul coal and pollute communities across the country with coal dust.\nStories about particulate pollution in the industrial sector have exposed environmental consequences for the air, earth, and our waterways. This project would take the first comprehensive look at communities along coal transportation routes – particularly relevant in the wake of major freight derailments, contamination events, and discussions about regulating the length and staff numbers aboard freight transport.\nThe potential impact of this project could be increased investment (social and financial) for particulate monitoring sensors along transportation hubs and/or increased industrial regulation for coal transporters. The social impact would be increasing the public awareness of industrial air pollution or providing an explanation for the phenomenon in affected communities.\nThis story would feature visual components like maps and other data graphics and photos and videos of coal trains and transportation routes.\nNext steps include: creating buffers and performing a spatial join to merge our demographic and geographic data. Then, I will take a look at demographic data at coal export locations nationwide. This analysis will precede a future analysis of complete train routes.\nThis page includes two parts: importing Census demographic data and mapping coal export locations. This test workflow will examine three communities where CSX rail lines run: Prince George’s County and Montgomery County in Maryland, and Jefferson County in West Virginia.\n\n\n\n\n# use this block for imports\nlibrary(leaflet) # for mapping\nlibrary(sf) # for mapping\nlibrary(janitor)\nlibrary(tidyverse)\nlibrary(tidycensus)"
  },
  {
    "objectID": "index.html#working-with-census-demographic-data",
    "href": "index.html#working-with-census-demographic-data",
    "title": "Where’s the coal? Export locations & demographics",
    "section": "Working with Census demographic data",
    "text": "Working with Census demographic data\n\n# Import demographic data\ncensus_county_stats = read.csv(\"data/census-county-stats.csv\")\n\n# List of counties you want to filter for\ncounties_to_keep &lt;- c(\"Prince George's County\", \"Montgomery County\", \"Jefferson County\")\n\n# List of states you want to filter for\nstates_to_keep &lt;- c(\"Maryland\", \"West Virginia\")\n\n# Filter the data frame\nfiltered_df &lt;- census_county_stats[census_county_stats$county %in% counties_to_keep & census_county_stats$state %in% states_to_keep, ]\n\n\n# Get state figures for comparison\n\nmaryland_stats &lt;- get_acs(geography = \"state\", variables = c( \"B01001_001\",\"B02001_002\",\"B02001_003\",\"B02001_004\",\"B03001_003\",\"B06012_002\",\"B19013_001\"), state=\"Maryland\", year = 2022) %&gt;%\n  select(GEOID, variable, estimate) %&gt;%\n  pivot_wider(names_from = variable, values_from = estimate) %&gt;%\n  rename(\n    total_pop = B01001_001,\n    white_pop = B02001_002,\n    black_pop = B02001_003,\n    native_pop = B02001_004,\n    hispanic_pop = B03001_003,\n    poverty_pop = B06012_002,\n    median_income = B19013_001\n  ) %&gt;%\n  mutate(pct_white = round(white_pop/total_pop, 2) * 100,\n         pct_nonwhite = 100 - round(white_pop/total_pop, 2) * 100,\n         pct_black = round(black_pop/total_pop, 2) * 100,\n         pct_native = round(native_pop/total_pop, 2) * 100,\n         pct_hispanic = round(hispanic_pop/total_pop, 2) * 100,\n         pct_poverty = round(poverty_pop/total_pop, 2) * 100\n         ) %&gt;%\n  clean_names()\n\n\nwva_stats &lt;- get_acs(geography = \"state\", variables = c( \"B01001_001\",\"B02001_002\",\"B02001_003\",\"B02001_004\",\"B03001_003\",\"B06012_002\",\"B19013_001\"), state=\"West Virginia\", year = 2022) %&gt;%\n  select(GEOID, variable, estimate) %&gt;%\n  pivot_wider(names_from = variable, values_from = estimate) %&gt;%\n  rename(\n    total_pop = B01001_001,\n    white_pop = B02001_002,\n    black_pop = B02001_003,\n    native_pop = B02001_004,\n    hispanic_pop = B03001_003,\n    poverty_pop = B06012_002,\n    median_income = B19013_001\n  ) %&gt;%\n  mutate(pct_white = round(white_pop/total_pop, 2) * 100,\n         pct_nonwhite = 100 - round(white_pop/total_pop, 2) * 100,\n         pct_black = round(black_pop/total_pop, 2) * 100,\n         pct_native = round(native_pop/total_pop, 2) * 100,\n         pct_hispanic = round(hispanic_pop/total_pop, 2) * 100,\n         pct_poverty = round(poverty_pop/total_pop, 2) * 100\n         ) %&gt;%\n  clean_names()\n\n \nTransport trains run through Hyattsville, Maryland (located in Prince George’s County), which is 14% white, 61% Black, 20% Hispanic, and has a 9% poverty rate. The median household income in PG County is $97935.\nIn Montgomery County (where Silver Spring is located), the population is 47% white, 19% Black, 20% Hispanic, and has a 7% poverty rate. The median household income in Montgomery County is $125,583.\nStatewide, the population is 51% white, 30% Black, 11% Hispanic, and has a 9% poverty rate. The median household income is $98,461.\n \nIn Jefferson County, West Virginia – where Harpers Ferry is located – the population is 84% white, 5% Black, 6% Hispanic, and has a 9% poverty rate. The median household income in Montgomery County is $93,744.\nStatewide, the population is 91% white, 3% Black, 2% Hispanic, and has a 16% poverty rate. The median household income in West Virginia is $55,217."
  },
  {
    "objectID": "index.html#mapping-coal-export-locations",
    "href": "index.html#mapping-coal-export-locations",
    "title": "Where’s the coal? Export locations & demographics",
    "section": "Mapping coal export locations",
    "text": "Mapping coal export locations\nIn Excel, I filtered the original global dataset to only coal export locations in the United States. That is the dataset that is imported here. The original data can be found and downloaded here.\nWe have our demographic data at a county and tract level, but we will be using counties for visualization since they render significantly faster in our browser. Tract-level data is available in the data folder.\n\n# import point and shapefiles for mapping\n\n# Load point data\ndata = read.csv(\"data/coal-export-terminals-us.csv\")\npoints_sf &lt;- st_as_sf(data, coords = c(\"Longitude\", \"Latitude\"), crs = 4269)\n\n# Load census county shapefiles\ncensus_counties &lt;- st_read(\"data/all_census_counties_us.shp\", quiet=TRUE)\n\n\n# Create the map\n# Quiet parameter turns off leaflet output text when it imports shapefile\nmap &lt;- leaflet(options = leafletOptions(preferCanvas = TRUE)) %&gt;%\n  addTiles() %&gt;%\n  setView(lng = -96.25, lat = 39.50, zoom = 4) %&gt;% \n  addPolygons(data = census_counties,\n              fillColor = \"lightgray\",\n              color = \"gray\",\n              weight = 1,\n              fillOpacity = 0.5) %&gt;% \n  addCircleMarkers(\n              data = points_sf,\n             color = \"red\",\n             opacity = 0.8,\n             weight = 2,\n             radius = 5)\n\n\nmap"
  },
  {
    "objectID": "tidycensus_data.html",
    "href": "tidycensus_data.html",
    "title": "County & Tract-level demographic data via TidyCensus",
    "section": "",
    "text": "About this page\nBelow, I have provided the code to produce dataframes with county and tract level demographic information, including race/ethnicity, poverty rate, and median income. These dataframes are stored in the census_tract_stats and census_county_stats variables. These dataframes are written to csv files.\nA future iteration of this code will functionalize the process of retrieving nationwide county and tract data in an R script.\n\n\nSet up libraries\n\nlibrary(tidyverse)\nlibrary(tidycensus)\nlibrary(janitor)\n\n\n# set api key for tidycensus\n# census_api_key(\"API_KEY\", install=TRUE)\n\n\n\nInstantiate global vars\n\n# instantiate global variable \ndata(fips_codes)\n\n## store to var\nfips_codes &lt;- fips_codes  \n\n\n\nRetrieve tract-level demographic data\n\n## Get list of states (Exclude non-states, except DC)\nstates &lt;- fips_codes %&gt;%\n  select(state) %&gt;%\n  distinct() %&gt;%\n  head(51) %&gt;%\n  as_vector() \n\n# Get census tract data for all states\ncensus_tract_stats &lt;- get_acs(geography = \"tract\", variables = c( \"B01001_001\",\"B02001_002\",\"B02001_003\",\"B02001_004\",\"B03001_003\",\"B06012_002\",\"B19013_001\"), state=states, year = 2022) %&gt;%\n  select(GEOID, variable, estimate) %&gt;%\n  pivot_wider(names_from = variable, values_from = estimate) %&gt;%\n  rename(\n    total_pop = B01001_001,\n    white_pop = B02001_002,\n    black_pop = B02001_003,\n    native_pop = B02001_004,\n    hispanic_pop = B03001_003,\n    poverty_pop = B06012_002,\n    median_income = B19013_001\n  ) %&gt;%\n  mutate(pct_white = round(white_pop/total_pop, 2) * 100,\n         pct_nonwhite = 100 - round(white_pop/total_pop, 2) * 100,\n         pct_black = round(black_pop/total_pop, 2) * 100,\n         pct_native = round(native_pop/total_pop, 2) * 100,\n         pct_hispanic = round(hispanic_pop/total_pop, 2) * 100,\n         pct_poverty = round(poverty_pop/total_pop, 2) * 100\n         ) %&gt;%\n  clean_names()\n\n# Extract state and county FIPS codes from GEOID\ncensus_tract_stats &lt;- census_tract_stats %&gt;%\n  mutate(state_fips = substr(geoid, 1, 2),\n         county_fips = substr(geoid, 3, 5))\n\n# Join with fips_codes to get state and county names\nfips_codes &lt;- fips_codes %&gt;%\n  select(state_fips = state_code, county_fips = county_code, state, county)\n\ncensus_tract_stats &lt;- census_tract_stats %&gt;%\n  left_join(fips_codes, by = c(\"state_fips\", \"county_fips\")) %&gt;%\n  select(geoid, state, county, starts_with(\"pct\"), median_income)\n\n\n# write dataframe to csv\nwrite.csv(census_tract_stats, file = \"data/census-tract-stats.csv\", row.names = FALSE)\n\n\n\nRetrieve county-level demographic data\n\n# Get county-level data for all states\ncensus_county_stats &lt;- get_acs(geography = \"county\", variables = c( \"B01001_001\",\"B02001_002\",\"B02001_003\",\"B02001_004\",\"B03001_003\",\"B06012_002\",\"B19013_001\"), year = 2022) %&gt;%\n  select(GEOID, NAME, variable, estimate) %&gt;%\n  pivot_wider(names_from = variable, values_from = estimate) %&gt;%\n  rename(\n    total_pop = B01001_001,\n    white_pop = B02001_002,\n    black_pop = B02001_003,\n    native_pop = B02001_004,\n    hispanic_pop = B03001_003,\n    poverty_pop = B06012_002,\n    median_income = B19013_001\n  ) %&gt;%\n  mutate(pct_white = round(white_pop/total_pop, 2) * 100,\n         pct_nonwhite = 100 - round(white_pop/total_pop, 2) * 100,\n         pct_black = round(black_pop/total_pop, 2) * 100,\n         pct_native = round(native_pop/total_pop, 2) * 100,\n         pct_hispanic = round(hispanic_pop/total_pop, 2) * 100,\n         pct_poverty = round(poverty_pop/total_pop, 2) * 100\n         ) %&gt;%\n  clean_names() %&gt;% \n  separate(name, into = c(\"county\", \"state\"), sep = \", \")\n\n\n# write dataframe to csv\nwrite.csv(census_county_stats, file = \"data/census-county-stats.csv\", row.names = FALSE)"
  },
  {
    "objectID": "coal_export_points.html",
    "href": "coal_export_points.html",
    "title": "Coal Export Locations",
    "section": "",
    "text": "About this page\nThis page provides code to create a basic Leaflet point map. A future version of this map/code will use county and tract level data that has been joined with Census demographic data.\n\n\nImport libraries\n\nlibrary(tigris)\nlibrary(sf)\nlibrary(leaflet)\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(tidycensus)\n\n\n\nData import\nIn Excel, I filtered the original global dataset to only coal export locations in the United States. That is the dataset that is imported here. The original data can be found and downloaded here.\n\ndata = read.csv(\"data/coal-export-terminals-us.csv\")\n\n\n\nDownload shapefiles for counties and tracts\n\n# Download the census tract shapefiles for all states from tigris package\n\n#census_tracts_us &lt;- tracts(cb=TRUE)\n\n#census_tracts_us &lt;-  st_transform(census_tracts_us, 4269)\n\n# save the tract level shapefiles to the working data directory\n# commenting out since file exists\n# st_write(census_tracts_us, \"data/all_census_tracts_us.shp\")\n\n# we're repeating the above process but with counties\n# keep the tract shapefiles in our data directory so we can do a more granular analysis locally\n\n#census_counties_us &lt;- counties(cb=TRUE)\n#census_counties_us &lt;-  st_transform(census_counties_us, 4269)\n\n# save county-level shapes to the working data directory\n# commenting out since file exists\n#st_write(census_counties_us, \"data/all_census_counties_us.shp\")\n\n\n\nInitialize a point map with Leaflet\n\n# Load point data\n\npoints_sf &lt;- st_as_sf(data, coords = c(\"Longitude\", \"Latitude\"), crs = 4269)\n\n# Load census tract shapefiles -- uncomment if you want a tract-level map\n#census_tracts &lt;- st_read(\"data/all_census_tracts_us.shp\")\n\n# Load census county shapefiles\ncensus_counties &lt;- st_read(\"data/all_census_counties_us.shp\")\n\n\n\nDraw the map\n\nmap &lt;- leaflet(options = leafletOptions(preferCanvas = TRUE)) %&gt;%\n  addTiles() %&gt;%\n  setView(lng = -96.25, lat = 39.50, zoom = 4) %&gt;% \n  addPolygons(data = census_counties,\n              fillColor = \"lightgray\",\n              color = \"gray\",\n              weight = 1,\n              fillOpacity = 0.5) %&gt;% \n  addCircleMarkers(\n              data = points_sf,\n             color = \"red\",\n             opacity = 0.8,\n             weight = 2,\n             radius = 5)\n\n\nmap"
  },
  {
    "objectID": "coal_export_points.html#import-libraries",
    "href": "coal_export_points.html#import-libraries",
    "title": "Coal Export Locations",
    "section": "",
    "text": "library(tigris)\nlibrary(sf)\nlibrary(leaflet)\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(tidycensus)"
  },
  {
    "objectID": "coal_export_points.html#data-import",
    "href": "coal_export_points.html#data-import",
    "title": "Coal Export Locations",
    "section": "data import",
    "text": "data import\nIn Excel, I filtered the original global dataset to only coal export locations in the United States. That is the dataset that is imported here. The original data can be found and downloaded here.\n\ndata = read.csv(\"data/coal-export-terminals-us.csv\")\nhead(data)\n\n  Terminal.ID                          Coal.Terminal.Name Coal.Terminal.AKAs\n1       T1346                          Arch Coal Terminal                   \n2       T1422                         Ashtabula Coal Pier                   \n3       T1424 Bulk Materials Handling Plant Coal Terminal                   \n4       T1403                      Burnside Coal Terminal                   \n5       T1335                           Burnside Terminal                   \n6       T1368                            Cahokia Terminal                   \n     Parent.Port                                           GEM.Wiki\n1              -            https://www.gem.wiki/Arch_Coal_Terminal\n2              -           https://www.gem.wiki/Ashtabula_Coal_Pier\n3 Port of Mobile https://www.gem.wiki/Bulk_Materials_Handling_Plant\n4              -             https://www.gem.wiki/Burnside_Terminal\n5              -             https://www.gem.wiki/Burnside_Terminal\n6              -              https://www.gem.wiki/Cahokia_Terminal\n  GEM.Wiki..Non.ENG.    Status Project.Type                        Owner\n1                 NA Operating                                 Arch Coal\n2                 NA   Retired                          Norfolk Southern\n3                 NA   Retired              Alabama State Port Authority\n4                 NA  Proposed    Expansion              Trafigura Group\n5                 NA Operating                           Trafigura Group\n6                 NA Operating                           Watco Companies\n  Capacity..Mt. Product.Type Terminal.Type Opening.Year Retire.Year\n1          5.44         Coal      Domestic            -           -\n2          5.72         Coal       Exports         1999        2016\n3          2.45         Coal      Domestic            -        2018\n4           6.8         Coal       Exports            -           -\n5           6.8         Coal       Exports         2014           -\n6             *         Coal      Domestic            -           -\n          Location State.Province       Country ISO.Code        Subregion\n1      Cattlesburg       Kentucky United States      840 Northern America\n2        Ashtabula           Ohio United States      840 Northern America\n3                         Alabama United States      840 Northern America\n4 Ascension Parish      Louisiana United States      840 Northern America\n5 Ascension Parish      Louisiana United States      840 Northern America\n6           Sauget       Illinois United States      840 Northern America\n    Region Latitude Longitude Accuracy\n1 Americas 38.30644 -82.57856    Exact\n2 Americas 41.87722 -80.79694    Exact\n3 Americas 30.72393 -88.04663    Exact\n4 Americas 30.13493 -90.92034    Exact\n5 Americas 30.13493 -90.92034    Exact\n6 Americas 38.60312 -90.18595    Exact\n                                                                 Coal.source\n1                                                                           \n2 Ohio, Pennsylvania and West Virginia, sent to Canada and Great Lakes Basin\n3                                                                           \n4                                                                           \n5                                                                           \n6                                                                           \n\n\n\n# Download the census tract shapefiles for all states from tigris package\n\n#census_tracts_us &lt;- tracts(cb=TRUE)\n\n#census_tracts_us &lt;-  st_transform(census_tracts_us, 4269)\n\n# save the tract level shapefiles to the working data directory\n# commenting out since file exists\n# st_write(census_tracts_us, \"data/all_census_tracts_us.shp\")\n\n# we're repeating the above process but with counties\n# keep the tract shapefiles in our data directory so we can do a more granular analysis locally\n\n#census_counties_us &lt;- counties(cb=TRUE)\n#census_counties_us &lt;-  st_transform(census_counties_us, 4269)\n\n# save county-level shapes to the working data directory\n# commenting out since file exists\n#st_write(census_counties_us, \"data/all_census_counties_us.shp\")\n\n\nInitialize a point map with Leaflet\n\n# Load point data\n\npoints_sf &lt;- st_as_sf(data, coords = c(\"Longitude\", \"Latitude\"), crs = 4269)\n\n# Load census tract shapefiles\n#census_tracts &lt;- st_read(\"data/all_census_tracts_us.shp\")\n\n# Load census county shapefiles\ncensus_counties &lt;- st_read(\"data/all_census_counties_us.shp\")\n\n\n# Create the map\nmap &lt;- leaflet(options = leafletOptions(preferCanvas = TRUE)) %&gt;%\n  addTiles() %&gt;%\n  setView(lng = -96.25, lat = 39.50, zoom = 4) %&gt;% \n  addPolygons(data = census_counties,\n              fillColor = \"lightgray\",\n              color = \"gray\",\n              weight = 1,\n              fillOpacity = 0.5) %&gt;% \n  addCircleMarkers(\n              data = points_sf,\n             color = \"red\",\n             opacity = 0.8,\n             weight = 2,\n             radius = 5)\n\n\nmap"
  },
  {
    "objectID": "coal_exports.html",
    "href": "coal_exports.html",
    "title": "Coal Export Locations",
    "section": "",
    "text": "About this page\nThis page provides code to create a basic Leaflet point map. A future version of this map/code will use county and tract level data that has been joined with Census demographic data.\n\n\nImport libraries\n\nlibrary(tigris)\nlibrary(sf)\nlibrary(leaflet)\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(tidycensus)\n\n\n\nData import\nIn Excel, I filtered the original global dataset to only coal export locations in the United States. That is the dataset that is imported here. The original data can be found and downloaded here.\n\ndata = read.csv(\"data/coal-export-terminals-us.csv\")\n\n\n\nDownload shapefiles for counties and tracts\n\n# Download the census tract shapefiles for all states from tigris package\n\n#census_tracts_us &lt;- tracts(cb=TRUE)\n\n#census_tracts_us &lt;-  st_transform(census_tracts_us, 4269)\n\n# save the tract level shapefiles to the working data directory\n# commenting out since file exists\n# st_write(census_tracts_us, \"data/all_census_tracts_us.shp\")\n\n# we're repeating the above process but with counties\n# keep the tract shapefiles in our data directory so we can do a more granular analysis locally\n\n#census_counties_us &lt;- counties(cb=TRUE)\n#census_counties_us &lt;-  st_transform(census_counties_us, 4269)\n\n# save county-level shapes to the working data directory\n# commenting out since file exists\n#st_write(census_counties_us, \"data/all_census_counties_us.shp\")\n\n\n\nInitialize a point map with Leaflet\n\n# Load point data\n\npoints_sf &lt;- st_as_sf(data, coords = c(\"Longitude\", \"Latitude\"), crs = 4269)\n\n# Load census tract shapefiles -- uncomment if you want a tract-level map\n#census_tracts &lt;- st_read(\"data/all_census_tracts_us.shp\")\n\n# Load census county shapefiles\ncensus_counties &lt;- st_read(\"data/all_census_counties_us.shp\")\n\n\n\nDraw the map\n\nmap &lt;- leaflet(options = leafletOptions(preferCanvas = TRUE)) %&gt;%\n  addTiles() %&gt;%\n  setView(lng = -96.25, lat = 39.50, zoom = 4) %&gt;% \n  addPolygons(data = census_counties,\n              fillColor = \"lightgray\",\n              color = \"gray\",\n              weight = 1,\n              fillOpacity = 0.5) %&gt;% \n  addCircleMarkers(\n              data = points_sf,\n             color = \"red\",\n             opacity = 0.8,\n             weight = 2,\n             radius = 5)\n\n\nmap"
  },
  {
    "objectID": "tidycensus_data.knit.html",
    "href": "tidycensus_data.knit.html",
    "title": "Coal Dust Project",
    "section": "",
    "text": "title: “County & Tract-level demographic data via TidyCensus”\nformat:\nhtml:\ntheme: cosmo\n\n\nAbout this page\nBelow, I have provided the code to produce dataframes with county and tract level demographic information, including race/ethnicity, poverty rate, and median income. These dataframes are stored in the census_tract_stats and census_county_stats variables. These dataframes are written to csv files.\nA future iteration of this code will functionalize the process of retrieving nationwide county and tract data in an R script.\n\n\nSet up libraries\n\nlibrary(tidyverse)\n\n\nlibrary(tidycensus)\n\n\nlibrary(janitor)\n\n\n# set api key for tidycensus\n\n\n# census_api_key(\"API_KEY\", install=TRUE)\n\n\n\nInstantiate global vars\n\n# instantiate global variable \n\n\ndata(fips_codes)\n\n\n\n\n\n## store to var\n\n\nfips_codes &lt;- fips_codes  \n\n\n\nRetrieve tract-level demographic data\n\n## Get list of states (Exclude non-states, except DC)\n\n\nstates &lt;- fips_codes %&gt;%\n\n\n  select(state) %&gt;%\n\n\n  distinct() %&gt;%\n\n\n  head(51) %&gt;%\n\n\n  as_vector() \n\n\n\n\n\n# Get census tract data for all states\n\n\ncensus_tract_stats &lt;- get_acs(geography = \"tract\", variables = c( \"B01001_001\",\"B02001_002\",\"B02001_003\",\"B02001_004\",\"B03001_003\",\"B06012_002\",\"B19013_001\"), state=states, year = 2022) %&gt;%\n\n\n  select(GEOID, variable, estimate) %&gt;%\n\n\n  pivot_wider(names_from = variable, values_from = estimate) %&gt;%\n\n\n  rename(\n\n    total_pop = B01001_001,\n\n    white_pop = B02001_002,\n\n    black_pop = B02001_003,\n\n    native_pop = B02001_004,\n\n    hispanic_pop = B03001_003,\n\n    poverty_pop = B06012_002,\n\n    median_income = B19013_001\n\n\n  ) %&gt;%\n\n\n  mutate(pct_white = round(white_pop/total_pop, 2) * 100,\n\n         pct_nonwhite = 100 - round(white_pop/total_pop, 2) * 100,\n\n         pct_black = round(black_pop/total_pop, 2) * 100,\n\n         pct_native = round(native_pop/total_pop, 2) * 100,\n\n         pct_hispanic = round(hispanic_pop/total_pop, 2) * 100,\n\n         pct_poverty = round(poverty_pop/total_pop, 2) * 100\n\n         ) %&gt;%\n\n\n  clean_names()\n\n\n\n\n\n# Extract state and county FIPS codes from GEOID\n\n\ncensus_tract_stats &lt;- census_tract_stats %&gt;%\n\n\n  mutate(state_fips = substr(geoid, 1, 2),\n\n         county_fips = substr(geoid, 3, 5))\n\n\n\n\n\n# Join with fips_codes to get state and county names\n\n\nfips_codes &lt;- fips_codes %&gt;%\n\n\n  select(state_fips = state_code, county_fips = county_code, state, county)\n\n\n\n\n\ncensus_tract_stats &lt;- census_tract_stats %&gt;%\n\n\n  left_join(fips_codes, by = c(\"state_fips\", \"county_fips\")) %&gt;%\n\n\n  select(geoid, state, county, starts_with(\"pct\"), median_income)\n\n\n# write dataframe to csv\n\n\nwrite.csv(census_tract_stats, file = \"data/census-tract-stats.csv\", row.names = FALSE)\n\n\n\nRetrieve county-level demographic data\n\n# Get county-level data for all states\n\n\ncensus_county_stats &lt;- get_acs(geography = \"county\", variables = c( \"B01001_001\",\"B02001_002\",\"B02001_003\",\"B02001_004\",\"B03001_003\",\"B06012_002\",\"B19013_001\"), year = 2022) %&gt;%\n\n\n  select(GEOID, NAME, variable, estimate) %&gt;%\n\n\n  pivot_wider(names_from = variable, values_from = estimate) %&gt;%\n\n\n  rename(\n\n    total_pop = B01001_001,\n\n    white_pop = B02001_002,\n\n    black_pop = B02001_003,\n\n    native_pop = B02001_004,\n\n    hispanic_pop = B03001_003,\n\n    poverty_pop = B06012_002,\n\n    median_income = B19013_001\n\n\n  ) %&gt;%\n\n\n  mutate(pct_white = round(white_pop/total_pop, 2) * 100,\n\n         pct_nonwhite = 100 - round(white_pop/total_pop, 2) * 100,\n\n         pct_black = round(black_pop/total_pop, 2) * 100,\n\n         pct_native = round(native_pop/total_pop, 2) * 100,\n\n         pct_hispanic = round(hispanic_pop/total_pop, 2) * 100,\n\n         pct_poverty = round(poverty_pop/total_pop, 2) * 100\n\n         ) %&gt;%\n\n\n  clean_names() %&gt;% \n\n\n  separate(name, into = c(\"county\", \"state\"), sep = \", \")\n\n\n# write dataframe to csv\n\n\nwrite.csv(census_county_stats, file = \"data/census-county-stats.csv\", row.names = FALSE)"
  }
]